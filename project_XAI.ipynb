{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44d9906",
   "metadata": {},
   "source": [
    "## Project Overview: *Static Images Network Analyzer*\n",
    "\n",
    "This is the final project for the Cognitive Learning course, titled *Static Images Network Analyzer*.  \n",
    "The goal is to analyze **cognitive biases** in image classification models, focusing on **spurious correlations**—for example, when the model is influenced more by background context than by the actual object in the image.\n",
    "\n",
    "The project has two main phases:\n",
    "1. **Controlled dataset generation**: we created artificial images combining neutral objects with potentially bias-inducing visual contexts, using *Stable Diffusion v1.5*.  \n",
    "   > The image generation code is located in the file: `generate_dataset_diff_v1_5.py.py`.\n",
    "2. **Model analysis**: we evaluated how three pretrained classifiers (AlexNet, ResNet-18, ViT-18) responded to these images and measured whether their predictions aligned with the original prompt or were misled by context.  \n",
    "   > All analysis and evaluation steps are implemented in this notebook.\n",
    "\n",
    "We extract the **top-10 logits** for each image-model pair, and send both the original prompt and the predictions to a **language model (LLM)** for semantic auditing. The LLM provides a coherence score (0–1), a short explanation, and optional confidence. These evaluations are stored in a `.jsonl` file.\n",
    "\n",
    "Finally, we build an aggregated **bias report** using precomputed statistics and LLM-generated justifications. This final Markdown report includes:\n",
    "- Aggregate performance metrics\n",
    "- Recurring error patterns\n",
    "- Detailed list of incoherent predictions\n",
    "- Class-specific logit behavior\n",
    "- Overall model verdict\n",
    "\n",
    "This workflow allows us to systematically study the effect of **spurious visual cues** and assess the **robustness and reliability** of vision models through a cognitively informed pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ae04aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q openai pandas pyarrow pillow tqdm urllib3 pycocotools requests torch torchvision python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec18c1",
   "metadata": {},
   "source": [
    "### CONFIGURATION: load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2f68a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION_MODEL: vit_b_16\n",
      "LLM_MODEL: gpt-4o-mini\n",
      "OPENAI_API_KEY: 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\n",
      "IMG_DIR: dataset/images\n",
      "META_CSV: dataset/dataset_metadata.csv\n",
      "OUTPUT_DIR: analysis_vit_b_16_1\n",
      "COHERENCE_THRESHOLD: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carica da file .env se presente\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Variabili configurabili\n",
    "VISION_MODEL = os.getenv(\"VISION_MODEL\", \"alexnet\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "IMG_DIR = Path(os.getenv(\"IMG_DIR\", \"dataset_/images\"))\n",
    "META_CSV = Path(os.getenv(\"META_CSV\", \"dataset_/dataset_metadata.csv\"))\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", \"analysis_out_\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "COHERENCE_THRESHOLD = float(os.getenv(\"COHERENCE_TH\", 0.5))\n",
    "TARGET_CLASSES = os.getenv(\"TARGET_CLASSES\", \"pillow,toilet seat,park bench,laptop,fox squirrel,tennis ball\").split(\",\")\n",
    "\n",
    "print(f\"VISION_MODEL: {VISION_MODEL}\")\n",
    "print(f\"LLM_MODEL: {LLM_MODEL}\")\n",
    "print(f\"OPENAI_API_KEY: {OPENAI_API_KEY}\")\n",
    "print(f\"IMG_DIR: {IMG_DIR}\")\n",
    "print(f\"META_CSV: {META_CSV}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"COHERENCE_THRESHOLD: {COHERENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334f0b1",
   "metadata": {},
   "source": [
    "### Load view model and ImageNet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bd4a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Caricamento modello dinamico\n",
    "model = getattr(models, VISION_MODEL)(pretrained=True).eval().to(device)\n",
    "\n",
    "# Etichette ImageNet\n",
    "import urllib.request\n",
    "labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "imagenet_labels = urllib.request.urlopen(labels_url).read().decode().splitlines()\n",
    "idx2label = {i: l for i, l in enumerate(imagenet_labels)}\n",
    "\n",
    "# Trasformazioni immagine\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111376e",
   "metadata": {},
   "source": [
    "### Top-10 Extraction\n",
    "\n",
    "The choice of 10 is not arbitrary.  \n",
    "Studies on ImageNet show that the **top ten logits** account for, on average, **over 95% of the softmax probability mass** in models such as **ResNet-50** or **ViT-B/16**.  \n",
    "This means that, in most cases, the remaining classes beyond the 10th position **contribute minimally to the overall semantic representation**.\n",
    "\n",
    "> Source: [arXiv:2206.07290](https://arxiv.org/pdf/2206.07290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7db62bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:08<00:00, 14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Esempi di top‑10 logits:\n",
      "\n",
      "📌 dataset/images/bookjacket__classroom__001.png\n",
      "Prompt: A neutral bookjacket in a classroom background\n",
      "Top‑10 logits (softmax):\n",
      "  - 549: envelope (0.388)\n",
      "  - 767: rubber eraser (0.129)\n",
      "  - 769: rule (0.095)\n",
      "  - 446: binder (0.036)\n",
      "  - 921: book jacket (0.018)\n",
      "  - 418: ballpoint (0.018)\n",
      "  - 624: library (0.013)\n",
      "  - 419: Band Aid (0.012)\n",
      "  - 893: wallet (0.008)\n",
      "  - 623: letter opener (0.008)\n",
      "Logits classi target:\n",
      "  - 721: pillow = 1.01\n",
      "  - 861: toilet seat = -0.11\n",
      "  - 703: park bench = 0.61\n",
      "  - 620: laptop = 1.22\n",
      "  - 335: fox squirrel = -0.63\n",
      "  - 852: tennis ball = -1.31\n",
      "\n",
      "📌 dataset/images/bookjacket__classroom__002.png\n",
      "Prompt: A neutral bookjacket in a classroom background\n",
      "Top‑10 logits (softmax):\n",
      "  - 549: envelope (0.399)\n",
      "  - 893: wallet (0.088)\n",
      "  - 921: book jacket (0.084)\n",
      "  - 446: binder (0.053)\n",
      "  - 563: fountain pen (0.048)\n",
      "  - 418: ballpoint (0.045)\n",
      "  - 623: letter opener (0.032)\n",
      "  - 769: rule (0.017)\n",
      "  - 681: notebook (0.015)\n",
      "  - 583: guillotine (0.012)\n",
      "Logits classi target:\n",
      "  - 721: pillow = -0.29\n",
      "  - 861: toilet seat = -0.70\n",
      "  - 703: park bench = -0.77\n",
      "  - 620: laptop = 2.95\n",
      "  - 335: fox squirrel = -0.12\n",
      "  - 852: tennis ball = -0.86\n",
      "\n",
      "📌 dataset/images/bookjacket__garage__002.png\n",
      "Prompt: A neutral bookjacket in a garage background\n",
      "Top‑10 logits (softmax):\n",
      "  - 587: hammer (0.764)\n",
      "  - 784: screwdriver (0.146)\n",
      "  - 477: carpenter's kit (0.011)\n",
      "  - 754: radio (0.005)\n",
      "  - 783: screw (0.003)\n",
      "  - 473: can opener (0.001)\n",
      "  - 896: washbasin (0.001)\n",
      "  - 596: hatchet (0.001)\n",
      "  - 707: pay-phone (0.001)\n",
      "  - 648: medicine chest (0.001)\n",
      "Logits classi target:\n",
      "  - 721: pillow = 0.35\n",
      "  - 861: toilet seat = 0.38\n",
      "  - 703: park bench = -0.31\n",
      "  - 620: laptop = 1.13\n",
      "  - 335: fox squirrel = -0.71\n",
      "  - 852: tennis ball = -0.60\n",
      "\n",
      "📌 dataset/images/bookjacket__green__001.png\n",
      "Prompt: A neutral bookjacket in a green background\n",
      "Top‑10 logits (softmax):\n",
      "  - 921: book jacket (0.743)\n",
      "  - 446: binder (0.186)\n",
      "  - 620: laptop (0.011)\n",
      "  - 549: envelope (0.007)\n",
      "  - 681: notebook (0.006)\n",
      "  - 592: hard disc (0.006)\n",
      "  - 917: comic book (0.002)\n",
      "  - 893: wallet (0.001)\n",
      "  - 922: menu (0.001)\n",
      "  - 481: cassette (0.001)\n",
      "Logits classi target:\n",
      "  - 721: pillow = -1.38\n",
      "  - 861: toilet seat = -0.61\n",
      "  - 703: park bench = 0.56\n",
      "  - 620: laptop = 5.86\n",
      "  - 335: fox squirrel = -0.20\n",
      "  - 852: tennis ball = 0.32\n",
      "\n",
      "📌 dataset/images/bookjacket__green__002.png\n",
      "Prompt: A neutral bookjacket in a green background\n",
      "Top‑10 logits (softmax):\n",
      "  - 549: envelope (0.470)\n",
      "  - 921: book jacket (0.178)\n",
      "  - 446: binder (0.167)\n",
      "  - 692: packet (0.086)\n",
      "  - 749: quill (0.004)\n",
      "  - 922: menu (0.003)\n",
      "  - 769: rule (0.002)\n",
      "  - 868: tray (0.002)\n",
      "  - 623: letter opener (0.002)\n",
      "  - 741: prayer rug (0.002)\n",
      "Logits classi target:\n",
      "  - 721: pillow = 0.51\n",
      "  - 861: toilet seat = -1.08\n",
      "  - 703: park bench = -0.16\n",
      "  - 620: laptop = 0.51\n",
      "  - 335: fox squirrel = 0.04\n",
      "  - 852: tennis ball = 0.15\n",
      "\n",
      "📌 dataset/images/bookjacket__hotel__001.png\n",
      "Prompt: A neutral bookjacket in a hotel background\n",
      "Top‑10 logits (softmax):\n",
      "  - 921: book jacket (0.336)\n",
      "  - 591: handkerchief (0.167)\n",
      "  - 789: shoji (0.124)\n",
      "  - 446: binder (0.062)\n",
      "  - 563: fountain pen (0.020)\n",
      "  - 749: quill (0.017)\n",
      "  - 846: table lamp (0.008)\n",
      "  - 549: envelope (0.007)\n",
      "  - 619: lampshade (0.007)\n",
      "  - 893: wallet (0.007)\n",
      "Logits classi target:\n",
      "  - 721: pillow = 1.03\n",
      "  - 861: toilet seat = 0.31\n",
      "  - 703: park bench = -0.20\n",
      "  - 620: laptop = 1.79\n",
      "  - 335: fox squirrel = -0.04\n",
      "  - 852: tennis ball = 0.13\n",
      "\n",
      "📌 dataset/images/bookjacket__hotel__002.png\n",
      "Prompt: A neutral bookjacket in a hotel background\n",
      "Top‑10 logits (softmax):\n",
      "  - 905: window shade (0.061)\n",
      "  - 515: cowboy hat (0.051)\n",
      "  - 619: lampshade (0.038)\n",
      "  - 859: toaster (0.036)\n",
      "  - 789: shoji (0.033)\n",
      "  - 673: mouse (0.029)\n",
      "  - 968: cup (0.026)\n",
      "  - 714: pick (0.023)\n",
      "  - 846: table lamp (0.019)\n",
      "  - 534: dishwasher (0.017)\n",
      "Logits classi target:\n",
      "  - 721: pillow = 0.90\n",
      "  - 861: toilet seat = 1.30\n",
      "  - 703: park bench = -0.60\n",
      "  - 620: laptop = 2.04\n",
      "  - 335: fox squirrel = -0.23\n",
      "  - 852: tennis ball = 0.61\n",
      "\n",
      "📌 dataset/images/bookjacket__kitchen__002.png\n",
      "Prompt: A neutral bookjacket in a kitchen background\n",
      "Top‑10 logits (softmax):\n",
      "  - 921: book jacket (0.516)\n",
      "  - 681: notebook (0.245)\n",
      "  - 446: binder (0.066)\n",
      "  - 620: laptop (0.019)\n",
      "  - 868: tray (0.013)\n",
      "  - 778: scale (0.008)\n",
      "  - 549: envelope (0.006)\n",
      "  - 662: modem (0.005)\n",
      "  - 592: hard disc (0.004)\n",
      "  - 831: studio couch (0.003)\n",
      "Logits classi target:\n",
      "  - 721: pillow = -0.87\n",
      "  - 861: toilet seat = 0.84\n",
      "  - 703: park bench = -0.30\n",
      "  - 620: laptop = 5.27\n",
      "  - 335: fox squirrel = 0.02\n",
      "  - 852: tennis ball = -0.09\n",
      "\n",
      "📌 dataset/images/bookjacket__minimalist__001.png\n",
      "Prompt: A neutral bookjacket in a minimalist background\n",
      "Top‑10 logits (softmax):\n",
      "  - 921: book jacket (0.469)\n",
      "  - 549: envelope (0.055)\n",
      "  - 446: binder (0.051)\n",
      "  - 767: rubber eraser (0.044)\n",
      "  - 868: tray (0.024)\n",
      "  - 563: fountain pen (0.021)\n",
      "  - 681: notebook (0.014)\n",
      "  - 648: medicine chest (0.013)\n",
      "  - 464: buckle (0.011)\n",
      "  - 626: lighter (0.009)\n",
      "Logits classi target:\n",
      "  - 721: pillow = -0.87\n",
      "  - 861: toilet seat = 1.45\n",
      "  - 703: park bench = 0.71\n",
      "  - 620: laptop = 1.36\n",
      "  - 335: fox squirrel = -0.10\n",
      "  - 852: tennis ball = -0.31\n",
      "\n",
      "📌 dataset/images/bookjacket__minimalist__002.png\n",
      "Prompt: A neutral bookjacket in a minimalist background\n",
      "Top‑10 logits (softmax):\n",
      "  - 921: book jacket (0.284)\n",
      "  - 446: binder (0.270)\n",
      "  - 681: notebook (0.057)\n",
      "  - 549: envelope (0.048)\n",
      "  - 563: fountain pen (0.025)\n",
      "  - 823: stethoscope (0.018)\n",
      "  - 592: hard disc (0.013)\n",
      "  - 605: iPod (0.009)\n",
      "  - 778: scale (0.005)\n",
      "  - 632: loudspeaker (0.005)\n",
      "Logits classi target:\n",
      "  - 721: pillow = -2.42\n",
      "  - 861: toilet seat = -1.26\n",
      "  - 703: park bench = 0.39\n",
      "  - 620: laptop = 2.47\n",
      "  - 335: fox squirrel = -0.13\n",
      "  - 852: tennis ball = -0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_topk(logits, k=10):\n",
    "    probs = torch.softmax(logits, dim=-1) ## corretto convertire in probabilità, motivare\n",
    "    top_p, top_i = torch.topk(probs, k)\n",
    "    return [(int(i), (idx2label[int(i)], float(p))) for p, i in zip(top_p.cpu(), top_i.cpu())]\n",
    "# Softmax is monotonic, so it can be used to rank logits, is more interpretable than raw logits for us and for LLM.\n",
    "\n",
    "def get_class_logits(logits, target_ids):\n",
    "    return {i: float(logits[i].cpu()) for i in target_ids}\n",
    "\n",
    "# load prompts from CSV\n",
    "prompts = {}\n",
    "with open(META_CSV, newline='') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        prompts[Path(row[\"file_name\"].strip()).name] = row[\"prompt\"]\n",
    "\n",
    "########### \n",
    "# target classes, for pt. 4 of the report  \n",
    "target_classes = TARGET_CLASSES\n",
    "labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "imagenet_labels = urllib.request.urlopen(labels_url).read().decode().splitlines()\n",
    "idx2label = {i: l for i, l in enumerate(imagenet_labels)}\n",
    "label2idx = {l: i for i, l in enumerate(imagenet_labels)}\n",
    "target_ids = {label2idx[c]: c for c in target_classes}\n",
    "per_class_logit = defaultdict(list)\n",
    "#########\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Image analysis\n",
    "for img_path in tqdm(sorted(IMG_DIR.glob(\"*.png\"))):\n",
    "    prompt = prompts.get(img_path.name)\n",
    "    if not prompt:\n",
    "        continue\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(transform(image).unsqueeze(0).to(device))[0]\n",
    "\n",
    "    # Softmax top‑10 logits\n",
    "    top_logits = get_topk(logits, k=10)\n",
    "\n",
    "    # Raw Logits of 6 target classes\n",
    "    selected_logits = get_class_logits(logits, target_ids.keys())\n",
    "\n",
    "    results.append({\n",
    "        \"file_name\": str(img_path),\n",
    "        \"prompt\": prompt,\n",
    "        \"top_logits\": top_logits,\n",
    "        \"class_logits\": selected_logits\n",
    "    })\n",
    "\n",
    "    # aggregate per class (for the dedicated report)\n",
    "    for cls_id, val in selected_logits.items():\n",
    "        per_class_logit[cls_id].append({\n",
    "            \"file_name\": str(img_path),\n",
    "            \"prompt\": prompt,\n",
    "            \"logit\": val\n",
    "        })\n",
    "\n",
    "# Debug/preview\n",
    "print(\"\\n✅ Esempi di top‑10 logits:\")\n",
    "for r in results[:10]:\n",
    "    print(f\"\\n📌 {r['file_name']}\")\n",
    "    print(f\"Prompt: {r['prompt']}\")\n",
    "    print(\"Top‑10 logits (softmax):\")\n",
    "    for i, (label, p) in r[\"top_logits\"]:\n",
    "        print(f\"  - {i}: {label} ({p:.3f})\")\n",
    "    print(\"Logits classi target:\")\n",
    "    for i, val in r[\"class_logits\"].items():\n",
    "        print(f\"  - {i}: {idx2label[i]} = {val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d479795",
   "metadata": {},
   "source": [
    "### LLM Coherence Audit \n",
    "\n",
    "This script audits the coherence between a prompt and a vision model's top-10 predictions using an LLM.\n",
    "\n",
    "- **`query_llm()`** sends the prompt and logits to an OpenAI model, which returns a JSON with a coherence `score`, a short `explanation`, and optional `confidence`.\n",
    "- A prediction is considered coherent if the score ≥ `COHERENCE_THRESHOLD` (default: 0.3).\n",
    "- Results are saved to `.jsonl` and `.txt` files.\n",
    "- The final report includes total examples, coherence rate, and breakdowns by object and context.\n",
    "\n",
    "This allows for automated semantic auditing of vision-language model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc0a408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHERENCE_THRESHOLD: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM analysis: 100%|██████████| 121/121 [03:09<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== SUMMARY ==========\n",
      "Total images:   121\n",
      "Incoherent (<0.5): 23  (19.0 %)\n",
      "\n",
      "-- Incoherence by *object* --\n",
      "  bookjacket                         : 4/15  (26.7 %)\n",
      "  ceramiccoffeemug                   : 0/16  (0.0 %)\n",
      "  grannysmith                        : 5/15  (33.3 %)\n",
      "  notebookwithkraftcover             : 6/15  (40.0 %)\n",
      "  opaquemetalwaterbottle             : 1/20  (5.0 %)\n",
      "  softcouchpillow                    : 1/20  (5.0 %)\n",
      "  tablelampwithshadeoff              : 6/20  (30.0 %)\n",
      "\n",
      "-- Incoherence by *context* --\n",
      "  bathroom                           : 4/8  (50.0 %)\n",
      "  classroom                          : 2/12  (16.7 %)\n",
      "  garage                             : 3/10  (30.0 %)\n",
      "  green                              : 5/12  (41.7 %)\n",
      "  hotel                              : 6/14  (42.9 %)\n",
      "  kitchen                            : 0/12  (0.0 %)\n",
      "  minimalist                         : 0/14  (0.0 %)\n",
      "  modern                             : 0/14  (0.0 %)\n",
      "  plain                              : 0/14  (0.0 %)\n",
      "  science                            : 3/11  (27.3 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai, json, re, os, sys\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "import csv\n",
    "\n",
    "# OpenAI API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Parameters\n",
    "COHERENCE_THRESHOLD = float(os.getenv(\"COHERENCE_TH\", 0.3))\n",
    "META_CSV = Path(os.getenv(\"META_CSV\", \"dataset/dataset_metadata.csv\"))\n",
    "LOG_JSONL_PATH = Path(os.getenv(\"OUTPUT_DIR\", \"analysis_out\")) / \"llm_audit.jsonl\"\n",
    "LIVE_TXT_PATH = Path(os.getenv(\"OUTPUT_DIR\", \"analysis_out\")) / \"llm_live_output.txt\"\n",
    "\n",
    "print(f\"COHERENCE_THRESHOLD: {COHERENCE_THRESHOLD}\")\n",
    "\n",
    "# Function to extract JSON from text\n",
    "def extract_json_from_text(text: str) -> dict:\n",
    "    m = re.search(r\"\\{.*?\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(\"⚠️ No valid JSON found\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "# Function to query the LLM\n",
    "def query_llm(prompt: str, top_logits, vision_model: str) -> dict:\n",
    "    def safe_label_prob(item):\n",
    "        try:\n",
    "            label = str(item[0])\n",
    "            prob_raw = item[1]\n",
    "            prob = float(prob_raw[0]) if isinstance(prob_raw, (tuple, list)) else float(prob_raw)\n",
    "            return f\"{label} ({prob:.3f})\"\n",
    "        except Exception:\n",
    "            return f\"[MALFORMED: {item}]\"\n",
    "\n",
    "    top_str = \"; \".join([safe_label_prob(it) for it in top_logits])\n",
    "\n",
    "    user_msg = f\"\"\"\n",
    "You are auditing the output of **{vision_model}** to assess alignment with the prompt.\n",
    "\n",
    "Prompt:\n",
    "\\\"{prompt}\\\"\n",
    "\n",
    "Top-10 predictions with probabilities:\n",
    "{top_str}\n",
    "\n",
    "Return JSON only:\n",
    "{{\n",
    "  \"score\": <float 0-1>,\n",
    "  \"explanation\": <≤25 words>,\n",
    "  \"confidence\": <float 0-1 (optional)>\n",
    "}}\n",
    "Be lenient; score ≥ 0.3 is considered coherent.\n",
    "\"\"\"\n",
    "\n",
    "    res = openai.chat.completions.create(\n",
    "        model=os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\"),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Return strict JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return extract_json_from_text(res.choices[0].message.content.strip())\n",
    "\n",
    "# Load dataset_metadata.csv\n",
    "metadata = []\n",
    "with open(META_CSV, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        metadata.append(row)\n",
    "\n",
    "# Variables for counting\n",
    "tot = tot_incoh = 0\n",
    "per_obj_tot = Counter()\n",
    "per_obj_incoh = Counter()\n",
    "per_ctx_tot = Counter()\n",
    "per_ctx_incoh = Counter()\n",
    "incoherent_cases = []\n",
    "\n",
    "# Main loop\n",
    "with open(LOG_JSONL_PATH, \"w\") as fout, open(LIVE_TXT_PATH, \"w\") as live:\n",
    "    for row in tqdm(metadata, desc=\"LLM analysis\"):\n",
    "        tot += 1\n",
    "        prompt = row[\"prompt\"]\n",
    "        obj = row[\"object\"]\n",
    "        ctx = row[\"background\"]\n",
    "\n",
    "        per_obj_tot[obj] += 1\n",
    "        per_ctx_tot[ctx] += 1\n",
    "\n",
    "        # Query LLM\n",
    "        llm_out = query_llm(prompt, row.get(\"top_logits\", []), os.getenv(\"VISION_MODEL\", \"alexnet\"))\n",
    "        record = {**row, **llm_out, \"subject\": obj, \"background\": ctx}\n",
    "\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        live.write(json.dumps({\n",
    "            \"id\": tot,\n",
    "            \"score\": llm_out.get(\"score\"),\n",
    "            \"explanation\": llm_out.get(\"explanation\")\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if llm_out.get(\"score\", 0.0) < COHERENCE_THRESHOLD:\n",
    "            incoherent_cases.append(record)\n",
    "            tot_incoh += 1\n",
    "            per_obj_incoh[obj] += 1\n",
    "            per_ctx_incoh[ctx] += 1\n",
    "\n",
    "# Final report\n",
    "print(\"\\n========== SUMMARY ==========\")\n",
    "pct = 100 * tot_incoh / tot if tot else 0\n",
    "print(f\"Total images:   {tot}\")\n",
    "print(f\"Incoherent (<{COHERENCE_THRESHOLD}): {tot_incoh}  ({pct:.1f} %)\")\n",
    "\n",
    "print(\"\\n-- Incoherence by *object* --\")\n",
    "for o in sorted(per_obj_tot):\n",
    "    pct = 100 * per_obj_incoh[o] / per_obj_tot[o] if per_obj_tot[o] else 0\n",
    "    print(f\"  {o:35s}: {per_obj_incoh[o]}/{per_obj_tot[o]}  ({pct:.1f} %)\")\n",
    "\n",
    "print(\"\\n-- Incoherence by *context* --\")\n",
    "for c in sorted(per_ctx_tot):\n",
    "    pct = 100 * per_ctx_incoh[c] / per_ctx_tot[c] if per_ctx_tot[c] else 0\n",
    "    print(f\"  {c:35s}: {per_ctx_incoh[c]}/{per_ctx_tot[c]}  ({pct:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae9b7a",
   "metadata": {},
   "source": [
    "### Bias Report & Model Verdict (Uses Pre-computed Metrics)\n",
    "\n",
    "This cell generates a bias analysis and verdict for the vision model based on previously computed LLM coherence scores and raw logits.\n",
    "\n",
    "- Loads all results from the audit (`llm_audit.jsonl`) and filters incoherent cases (`score` < threshold).\n",
    "- Computes global statistics: total images, mean/median/stdev scores, incoherence rates by object and context.\n",
    "- Analyzes raw logits per target class: average activations, top-5 examples.\n",
    "- Constructs a structured prompt for the LLM including:\n",
    "  - (A) global metrics\n",
    "  - (B) incoherent examples\n",
    "  - (C) target class activation stats\n",
    "- The LLM returns a detailed Markdown report with six required sections, including bias patterns and an overall model verdict.\n",
    "- Final report is saved as `report.md`.\n",
    "\n",
    "This step automates bias evaluation and model reliability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1d1a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Target Class Analysis (Raw Logits)\n",
      "\n",
      "### Class `pillow` (ImageNet #721)\n",
      "- Number of activations: 121\n",
      "- Average logit: 1.16 (std: 3.38)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/softcouchpillow__plain__001.png` → logit=9.78\n",
      "  - `dataset/images/softcouchpillow__kitchen__001.png` → logit=9.51\n",
      "  - `dataset/images/softcouchpillow__bathroom__001.png` → logit=9.41\n",
      "  - `dataset/images/softcouchpillow__kitchen__002.png` → logit=9.17\n",
      "  - `dataset/images/softcouchpillow__modern__002.png` → logit=9.12\n",
      "\n",
      "### Class `toilet seat` (ImageNet #861)\n",
      "- Number of activations: 121\n",
      "- Average logit: -0.19 (std: 1.08)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/ceramiccoffeemug__bathroom__001.png` → logit=4.79\n",
      "  - `dataset/images/softcouchpillow__minimalist__001.png` → logit=2.89\n",
      "  - `dataset/images/opaquemetalwaterbottle__bathroom__001.png` → logit=2.69\n",
      "  - `dataset/images/bookjacket__plain__002.png` → logit=2.59\n",
      "  - `dataset/images/ceramiccoffeemug__kitchen__001.png` → logit=1.91\n",
      "\n",
      "### Class `park bench` (ImageNet #703)\n",
      "- Number of activations: 121\n",
      "- Average logit: 0.06 (std: 0.56)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/softcouchpillow__science__001.png` → logit=1.83\n",
      "  - `dataset/images/tablelampwithshadeoff__garage__002.png` → logit=1.72\n",
      "  - `dataset/images/tablelampwithshadeoff__classroom__002.png` → logit=1.68\n",
      "  - `dataset/images/softcouchpillow__green__001.png` → logit=1.13\n",
      "  - `dataset/images/opaquemetalwaterbottle__green__002.png` → logit=1.01\n",
      "\n",
      "### Class `laptop` (ImageNet #620)\n",
      "- Number of activations: 121\n",
      "- Average logit: 0.81 (std: 1.37)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/bookjacket__green__001.png` → logit=5.86\n",
      "  - `dataset/images/notebookwithkraftcover__science__001.png` → logit=5.54\n",
      "  - `dataset/images/bookjacket__kitchen__002.png` → logit=5.27\n",
      "  - `dataset/images/ceramiccoffeemug__modern__001.png` → logit=4.38\n",
      "  - `dataset/images/notebookwithkraftcover__modern__001.png` → logit=4.26\n",
      "\n",
      "### Class `fox squirrel` (ImageNet #335)\n",
      "- Number of activations: 121\n",
      "- Average logit: -0.05 (std: 0.34)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/grannysmith__green__002.png` → logit=0.71\n",
      "  - `dataset/images/softcouchpillow__modern__002.png` → logit=0.70\n",
      "  - `dataset/images/softcouchpillow__science__002.png` → logit=0.59\n",
      "  - `dataset/images/grannysmith__bathroom__002.png` → logit=0.53\n",
      "  - `dataset/images/ceramiccoffeemug__kitchen__002.png` → logit=0.49\n",
      "\n",
      "### Class `tennis ball` (ImageNet #852)\n",
      "- Number of activations: 121\n",
      "- Average logit: 0.23 (std: 0.85)\n",
      "- Top‑5 activations:\n",
      "  - `dataset/images/grannysmith__hotel__002.png` → logit=3.52\n",
      "  - `dataset/images/grannysmith__green__002.png` → logit=3.39\n",
      "  - `dataset/images/grannysmith__classroom__001.png` → logit=2.46\n",
      "  - `dataset/images/grannysmith__minimalist__002.png` → logit=2.44\n",
      "  - `dataset/images/ceramiccoffeemug__green__002.png` → logit=2.03\n",
      "\n",
      "✅ Report saved to: analysis_vit_b_16_1/report.md\n"
     ]
    }
   ],
   "source": [
    "import json, openai, statistics, os\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# ── load all the records from previous cell ───────────────────\n",
    "with open(LOG_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = [json.loads(l) for l in f]\n",
    "\n",
    "# Limit top_logits to 5 for each record\n",
    "for rec in records:\n",
    "    rec[\"top_logits\"] = rec.get(\"top_logits\", [])[:5]\n",
    "\n",
    "# List of incoherent records (those with score < COHERENCE_THRESHOLD)\n",
    "incoherent_recs = [\n",
    "    {k: rec[k] for k in (\"file_name\", \"prompt\", \"top_logits\", \"score\", \"explanation\")}\n",
    "    for rec in records if rec.get(\"score\", 0) < COHERENCE_THRESHOLD\n",
    "]\n",
    "\n",
    "# ── global metrics already computed in previous cell ─────────────────────────\n",
    "scores = [rec.get(\"score\", 0.0) for rec in records]\n",
    "metrics_summary = {\n",
    "    \"total_images\": tot,\n",
    "    \"mean_score\": statistics.mean(scores) if scores else 0.0,\n",
    "    \"median_score\": statistics.median(scores) if scores else 0.0,\n",
    "    \"stdev_score\": statistics.pstdev(scores) if len(scores) > 1 else 0.0,\n",
    "    \"percent_incoherent\": 100 * tot_incoh / tot if tot else 0.0,\n",
    "    \"object_stats\": {\n",
    "        obj: {\n",
    "            \"total\": per_obj_tot[obj],\n",
    "            \"incoherent\": per_obj_incoh[obj],\n",
    "            \"percent_incoherent\": 100 * per_obj_incoh[obj] / per_obj_tot[obj]\n",
    "            if per_obj_tot[obj] else 0.0\n",
    "        }\n",
    "        for obj in per_obj_tot\n",
    "    },\n",
    "    \"context_stats\": {\n",
    "        ctx: {\n",
    "            \"total\": per_ctx_tot[ctx],\n",
    "            \"incoherent\": per_ctx_incoh[ctx],\n",
    "            \"percent_incoherent\": 100 * per_ctx_incoh[ctx] / per_ctx_tot[ctx]\n",
    "            if per_ctx_tot[ctx] else 0.0\n",
    "        }\n",
    "        for ctx in per_ctx_tot\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to file (may be useful)\n",
    "Path(OUTPUT_DIR /  \"metrics.json\").write_text(json.dumps(metrics_summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "logit_report_section = \"\\n## Target Class Analysis (Raw Logits)\\n\"\n",
    "for cls_id, cls_name in target_ids.items():\n",
    "    values = [x[\"logit\"] for x in per_class_logit[cls_id]]\n",
    "    if not values:\n",
    "        continue\n",
    "\n",
    "    logit_report_section += f\"\\n### Class `{cls_name}` (ImageNet #{cls_id})\\n\"\n",
    "    logit_report_section += f\"- Number of activations: {len(values)}\\n\"\n",
    "    logit_report_section += f\"- Average logit: {mean(values):.2f} (std: {stdev(values):.2f})\\n\"\n",
    "    logit_report_section += \"- Top‑5 activations:\\n\"\n",
    "    top5 = sorted(per_class_logit[cls_id], key=lambda x: -x[\"logit\"])[:5]\n",
    "    for e in top5:\n",
    "       logit_report_section += f\"  - `{e['file_name']}` → logit={e['logit']:.2f}\\n\"\n",
    "\n",
    "print(logit_report_section)\n",
    "\n",
    "# ── Prompt for LLM ──────\n",
    "prompt_header = f\"\"\"\n",
    "You are an AI-bias auditor.  \n",
    "Below you will find **(A) pre-computed global metrics**, **(B) per-image data**, and **(C) target class logit analysis**.\n",
    "\n",
    "Use the provided metrics; do NOT recalsculate means or percentages yourself.\n",
    "Respond in **Markdown** with the requested sections.\n",
    "\n",
    "## Required sections\n",
    "### 1 Aggregate statistics\n",
    "Summarise the numbers from (A).\n",
    "\n",
    "### 2 Recurring error patterns\n",
    "Identify frequent error types and link them to biases in **{VISION_MODEL}**.\n",
    "\n",
    "### 3 Detailed list of incoherent images\n",
    "For every image in (B) (score < {COHERENCE_THRESHOLD}) list:\n",
    "• file_name  • ≤15-word prompt summary  • three worst labels  • explanation (≤2 sentences).\n",
    "\n",
    "### 4 Target class logit analysis (Full Details)\n",
    "Include the full details of the target class analysis from (C).\n",
    "\n",
    "### 5 Main biases of the model\n",
    "At least three systematic biases, with examples.\n",
    "\n",
    "### 6 Overall verdict\n",
    "Bullet strengths/weaknesses of **{VISION_MODEL}** + final reliability rating 1–5 (no mitigation advice).\n",
    "\n",
    "Respond **only** in Markdown, start each major section with '##'.\n",
    "\"\"\"\n",
    "\n",
    "payload = (\n",
    "    prompt_header\n",
    "    + \"\\n\\n### (A) Global metrics\\n```json\\n\"\n",
    "    + json.dumps(metrics_summary, ensure_ascii=False, indent=2)\n",
    "    + \"\\n```\\n\\n### (B) Incoherent images\\n```json\\n\"\n",
    "    + json.dumps(incoherent_recs, ensure_ascii=False)\n",
    "    + \"\\n```\\n\\n### (C) Target class logit analysis (Full Details)\\n\"\n",
    "    + logit_report_section\n",
    ")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a senior AI-bias analyst who MUST reply in Markdown headings.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": payload}\n",
    "    ],\n",
    "    temperature=0.25\n",
    ")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "report_path = OUTPUT_DIR / \"report.md\"\n",
    "report_path.write_text(response.choices[0].message.content, encoding=\"utf-8\")\n",
    "print(\"✅ Report saved to:\", report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81372111",
   "metadata": {},
   "source": [
    "📺 Visualizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfa58426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report salvato in: analysis_vit_b_16_1/report.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 1 Aggregate statistics\n",
       "- **Total Images**: 121\n",
       "- **Mean Score**: 0.57\n",
       "- **Median Score**: 0.6\n",
       "- **Standard Deviation of Score**: 0.09\n",
       "- **Percentage of Incoherent Images**: 19.01%\n",
       "  \n",
       "### Object Incoherence Statistics\n",
       "- **Bookjacket**: 26.67% incoherent (4 out of 15)\n",
       "- **Ceramic Coffee Mug**: 0.0% incoherent (0 out of 16)\n",
       "- **Granny Smith**: 33.33% incoherent (5 out of 15)\n",
       "- **Notebook with Kraft Cover**: 40.0% incoherent (6 out of 15)\n",
       "- **Opaque Metal Water Bottle**: 5.0% incoherent (1 out of 20)\n",
       "- **Soft Couch Pillow**: 5.0% incoherent (1 out of 20)\n",
       "- **Table Lamp with Shade Off**: 30.0% incoherent (6 out of 20)\n",
       "\n",
       "### Context Incoherence Statistics\n",
       "- **Classroom**: 16.67% incoherent (2 out of 12)\n",
       "- **Garage**: 30.0% incoherent (3 out of 10)\n",
       "- **Green**: 41.67% incoherent (5 out of 12)\n",
       "- **Hotel**: 42.86% incoherent (6 out of 14)\n",
       "- **Kitchen**: 0.0% incoherent (0 out of 12)\n",
       "- **Minimalist**: 0.0% incoherent (0 out of 14)\n",
       "- **Modern**: 0.0% incoherent (0 out of 14)\n",
       "- **Plain**: 0.0% incoherent (0 out of 14)\n",
       "- **Science**: 27.27% incoherent (3 out of 11)\n",
       "- **Bathroom**: 50.0% incoherent (4 out of 8)\n",
       "\n",
       "## 2 Recurring error patterns\n",
       "- **Background Mismatch**: Many incoherent images feature objects that do not match the context of their backgrounds, such as book jackets in garages or bathrooms.\n",
       "- **Lack of Specificity**: The model often fails to provide specific details about the objects, leading to vague outputs that do not align with prompts.\n",
       "- **High Incoherence in Certain Classes**: Objects like \"notebook with kraft cover\" and \"granny smith\" show a higher percentage of incoherence, indicating a potential bias in how these objects are represented in various contexts.\n",
       "\n",
       "## 3 Detailed list of incoherent images\n",
       "1. **File Name**: images/bookjacket__garage__002.png  \n",
       "   **Prompt Summary**: A neutral bookjacket in a garage background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The predictions suggest a neutral design, but the garage background may not be fully represented.\n",
       "\n",
       "2. **File Name**: images/bookjacket__green__001.png  \n",
       "   **Prompt Summary**: A neutral bookjacket in a green background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The predictions suggest a green background, but lack specific details about the bookjacket's neutrality.\n",
       "\n",
       "3. **File Name**: images/bookjacket__hotel__001.png  \n",
       "   **Prompt Summary**: A neutral bookjacket in a hotel background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output suggests a hotel setting but lacks specific details about the bookjacket.\n",
       "\n",
       "4. **File Name**: images/bookjacket__hotel__002.png  \n",
       "   **Prompt Summary**: A neutral bookjacket in a hotel background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output suggests a neutral design but lacks specific hotel context.\n",
       "\n",
       "5. **File Name**: images/grannysmith__bathroom__002.png  \n",
       "   **Prompt Summary**: A neutral grannysmith in a bathroom background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output includes a grannysmith apple in a bathroom setting, aligning with the prompt's elements.\n",
       "\n",
       "6. **File Name**: images/grannysmith__classroom__001.png  \n",
       "   **Prompt Summary**: A neutral grannysmith in a classroom background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output suggests a grannysmith apple in a classroom, aligning with the prompt's elements.\n",
       "\n",
       "7. **File Name**: images/grannysmith__classroom__002.png  \n",
       "   **Prompt Summary**: A neutral grannysmith in a classroom background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The predictions suggest a focus on a grannysmith apple in a classroom, aligning with the prompt.\n",
       "\n",
       "8. **File Name**: images/grannysmith__hotel__001.png  \n",
       "   **Prompt Summary**: A neutral grannysmith in a hotel background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output suggests a neutral grannysmith in a hotel setting, aligning with the prompt's description.\n",
       "\n",
       "9. **File Name**: images/grannysmith__hotel__002.png  \n",
       "   **Prompt Summary**: A neutral grannysmith in a hotel background  \n",
       "   **Three Worst Labels**: None provided  \n",
       "   **Explanation**: The output likely includes a grannysmith apple in a hotel setting, aligning with the prompt.\n",
       "\n",
       "10. **File Name**: images/notebookwithkraftcover__garage__001.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a garage background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a notebook with a kraft cover, but the garage background is unclear.\n",
       "\n",
       "11. **File Name**: images/notebookwithkraftcover__green__001.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a green background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a neutral notebook with a kraft cover, aligning with the prompt's description.\n",
       "\n",
       "12. **File Name**: images/notebookwithkraftcover__hotel__001.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a hotel background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a notebook with a kraft cover, fitting the prompt's description.\n",
       "\n",
       "13. **File Name**: images/notebookwithkraftcover__hotel__002.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a hotel background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a notebook with a kraft cover, but lacks clear hotel context.\n",
       "\n",
       "14. **File Name**: images/notebookwithkraftcover__science__001.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a science background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a neutral notebook with a kraft cover, fitting the science theme.\n",
       "\n",
       "15. **File Name**: images/notebookwithkraftcover__science__002.png  \n",
       "    **Prompt Summary**: A neutral notebookwithkraftcover in a science background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a neutral notebook with a kraft cover, fitting the science background theme.\n",
       "\n",
       "16. **File Name**: images/opaquemetalwaterbottle__green__001.png  \n",
       "    **Prompt Summary**: A neutral opaquemetalwaterbottle in a green background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a water bottle on a green background, aligning with the prompt's description.\n",
       "\n",
       "17. **File Name**: images/softcouchpillow__bathroom__001.png  \n",
       "    **Prompt Summary**: A neutral softcouchpillow in a bathroom background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a soft couch pillow in a bathroom, aligning with the prompt's description.\n",
       "\n",
       "18. **File Name**: images/tablelampwithshadeoff__bathroom__001.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a bathroom background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output suggests a table lamp in a bathroom, aligning with the prompt's description.\n",
       "\n",
       "19. **File Name**: images/tablelampwithshadeoff__bathroom__002.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a bathroom background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output includes a table lamp but lacks clarity on the bathroom setting.\n",
       "\n",
       "20. **File Name**: images/tablelampwithshadeoff__garage__002.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a garage background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The predictions suggest a table lamp in a garage, aligning with the prompt's description.\n",
       "\n",
       "21. **File Name**: images/tablelampwithshadeoff__green__001.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a green background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output aligns with the prompt by depicting a table lamp with a shade against a green background.\n",
       "\n",
       "22. **File Name**: images/tablelampwithshadeoff__green__002.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a green background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The output aligns with the prompt by depicting a table lamp with a shade against a green background.\n",
       "\n",
       "23. **File Name**: images/tablelampwithshadeoff__science__001.png  \n",
       "    **Prompt Summary**: A neutral tablelampwithshadeoff in a science background  \n",
       "    **Three Worst Labels**: None provided  \n",
       "    **Explanation**: The predictions likely include elements of a lamp and a science background, aligning with the prompt.\n",
       "\n",
       "## 4 Target class logit analysis (Full Details)\n",
       "### Class `pillow` (ImageNet #721)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: 1.16 (std: 3.38)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/softcouchpillow__plain__001.png` → logit=9.78\n",
       "  - `dataset/images/softcouchpillow__kitchen__001.png` → logit=9.51\n",
       "  - `dataset/images/softcouchpillow__bathroom__001.png` → logit=9.41\n",
       "  - `dataset/images/softcouchpillow__kitchen__002.png` → logit=9.17\n",
       "  - `dataset/images/softcouchpillow__modern__002.png` → logit=9.12\n",
       "\n",
       "### Class `toilet seat` (ImageNet #861)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: -0.19 (std: 1.08)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/ceramiccoffeemug__bathroom__001.png` → logit=4.79\n",
       "  - `dataset/images/softcouchpillow__minimalist__001.png` → logit=2.89\n",
       "  - `dataset/images/opaquemetalwaterbottle__bathroom__001.png` → logit=2.69\n",
       "  - `dataset/images/bookjacket__plain__002.png` → logit=2.59\n",
       "  - `dataset/images/ceramiccoffeemug__kitchen__001.png` → logit=1.91\n",
       "\n",
       "### Class `park bench` (ImageNet #703)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: 0.06 (std: 0.56)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/softcouchpillow__science__001.png` → logit=1.83\n",
       "  - `dataset/images/tablelampwithshadeoff__garage__002.png` → logit=1.72\n",
       "  - `dataset/images/tablelampwithshadeoff__classroom__002.png` → logit=1.68\n",
       "  - `dataset/images/softcouchpillow__green__001.png` → logit=1.13\n",
       "  - `dataset/images/opaquemetalwaterbottle__green__002.png` → logit=1.01\n",
       "\n",
       "### Class `laptop` (ImageNet #620)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: 0.81 (std: 1.37)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/bookjacket__green__001.png` → logit=5.86\n",
       "  - `dataset/images/notebookwithkraftcover__science__001.png` → logit=5.54\n",
       "  - `dataset/images/bookjacket__kitchen__002.png` → logit=5.27\n",
       "  - `dataset/images/ceramiccoffeemug__modern__001.png` → logit=4.38\n",
       "  - `dataset/images/notebookwithkraftcover__modern__001.png` → logit=4.26\n",
       "\n",
       "### Class `fox squirrel` (ImageNet #335)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: -0.05 (std: 0.34)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/grannysmith__green__002.png` → logit=0.71\n",
       "  - `dataset/images/softcouchpillow__modern__002.png` → logit=0.70\n",
       "  - `dataset/images/softcouchpillow__science__002.png` → logit=0.59\n",
       "  - `dataset/images/grannysmith__bathroom__002.png` → logit=0.53\n",
       "  - `dataset/images/ceramiccoffeemug__kitchen__002.png` → logit=0.49\n",
       "\n",
       "### Class `tennis ball` (ImageNet #852)\n",
       "- **Number of activations**: 121\n",
       "- **Average logit**: 0.23 (std: 0.85)\n",
       "- **Top‑5 activations**:\n",
       "  - `dataset/images/grannysmith__hotel__002.png` → logit=3.52\n",
       "  - `dataset/images/grannysmith__green__002.png` → logit=3.39\n",
       "  - `dataset/images/grannysmith__classroom__001.png` → logit=2.46\n",
       "  - `dataset/images/grannysmith__minimalist__002.png` → logit=2.44\n",
       "  - `dataset/images/ceramiccoffeemug__green__002.png` → logit=2.03\n",
       "\n",
       "## 5 Main biases of the model\n",
       "- **Contextual Bias**: The model struggles with specific contexts, particularly in environments like bathrooms and garages, leading to incoherent outputs.\n",
       "- **Object Representation Bias**: Certain objects, such as \"notebook with kraft cover\" and \"granny smith,\" show higher incoherence rates, indicating a bias in representation.\n",
       "- **Background Misalignment**: The model frequently misaligns objects with their backgrounds, resulting in outputs that do not match the prompts effectively.\n",
       "\n",
       "## 6 Overall verdict\n",
       "- **Strengths**:\n",
       "  - Generally good performance with a mean score of 0.57.\n",
       "  - Some object classes, like \"ceramic coffee mug,\" show no incoherence.\n",
       "  \n",
       "- **Weaknesses**:\n",
       "  - High incoherence rates in specific classes and contexts.\n",
       "  - Frequent background misalignment leading to incoherent outputs.\n",
       "\n",
       "**Final Reliability Rating**: 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Output finale\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(\"Report salvato in:\", OUTPUT_DIR / \"report.md\")\n",
    "report_md = report_path.read_text(encoding=\"utf-8\")\n",
    "display(Markdown(report_md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
